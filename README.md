This project, titled Sign2Language, is a real-time sign language recognition and translation system designed to bridge the communication gap between individuals who use sign language and those who rely on spoken or written language. The system captures live video input through a camera and processes hand gestures and movements in real time using Computer Vision and Deep Learning techniques, with a Convolutional Neural Network (CNN) serving as the core model for gesture recognition. Through extensive preprocessing steps such as frame extraction, hand detection, background removal, normalization, and feature enhancement, the system accurately identifies individual signs from continuous motion. Recognized gestures are then mapped to corresponding words and dynamically combined to form meaningful sentences, enabling natural and contextual translation rather than isolated character output. The project leverages multiple supporting libraries for video processing, image manipulation, real-time inference, and sequence handling to ensure smooth and responsive performance. By focusing on real-time recognition, the system allows users to sign naturally without interruption, making the interaction intuitive and practical for everyday communication. Sign2Language not only demonstrates the effective application of CNN-based models in gesture recognition but also highlights the potential of assistive technologies to promote accessibility and inclusivity, with future scope for supporting multiple sign languages, improving sentence grammar using language models, integrating text-to-speech output, and deploying the system as a web or mobile application for wider real-world use.
